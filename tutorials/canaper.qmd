---
title: "Spatial phylogenetics in R with canaper"
format: gfm
execute: 
  echo: true
  eval: true
  cache: true
output-dir: tutorials
---

This tutorial demonstrates how to use [`canaper`](https://github.com/ropensci/canaper) to conduct spatial phylogenetic analysis in R.

## Load data

Normally you would probably load data into R using a function like `read.csv()` for grid-cell data and `ape::read.tree()` for the phylogeny. However, `canaper` comes pre-loaded with some example data, so we will use that for the tutorial. The data files are available after you have loaded `canaper`. We will also load the `tidyverse` set of packages for handling data and creating plots.

First, load packages.

```{r}
#| label: setup
#| message: false
#| warning: false
#| cache: false
library(canaper)
library(tidyverse)
```

Next, load data. We will use two example datasets for this tutorial, `biod_example` and `acacia`. `biod_example` is a small example dataset of made-up data for demonstration and testing purposes. `acacia` is a real dataset of *Acacia* species in Australia (Mishler et al. 2014).

```{r}
#| label: data
data(biod_example)
data(acacia)
```

Each dataset is a list including two items. The first, `phy`, is the phylogeny:

```{r}
#| label: inspect-data-phy
biod_example$phy
acacia$phy
```

The second, `comm`, is a dataframe with OTUs as columns and rows as sites. The row names (sites) correspond to grid-cell centroids. For `biod_example` they are just made up; for `acacia` they are of 50 x 50 km grid cells covering Australia. The dataframe is too large to print out in its entirety, so we will just take a look at the first 8 rows and columns^[You might think the dataset is all zeros, but that is not the case. It is just very sparse.]:

```{r}
#| label: inspect-data-comm
dim(biod_example$comm)
biod_example$comm[1:8, 1:8]

dim(acacia$comm)
acacia$comm[1:8, 1:8]
```

## About randomizations

As we learned in the workshop, randomizations are a key part of spatial phylogenetics analysis that allow us to determine if the observed phylogenetic structure is different from our null hypothesis.

The statistical expectation is largely **determined by the settings used for the randomization**. So we must take great care to use a randomization that makes **biological sense** for our study.

One kind of commonly used randomization algorithm (also sometimes referred to as the "null model") preserves marginal sums; in other words, the total number of species in each site and total number of occurrences of each species is unchanged, but the distribution of species across sites is shuffled. Generally, when testing hypotheses we only want to change one variable at a time, so it is probably good to leave abundance untouched while manipulating distribution patterns. But this might not always be the case depending on your study (you might want to manipulate something else). So think about it!

Anyways, we will use a randomization algorithm called "curveball" that preserves marginal sums while shuffling occurrences. The next step is to determine settings to use.

`cpr_rand_test()` includes two settings, `n_reps` and `n_iterations`. These sound similar but refer to two very different things. 

`n_reps` is the number of random grid-cells to simulate. For example, if `n_reps` is 100, will we be comparing each observed value (e.g., phylogenetic diversity, `pd_obs`), with 100 random replicates of `pd_obs`. If `n_reps` is too low, we will lack sufficiently statistical power to detect patterns in the data.

`n_iterations` is only used by some randomization algorithms, the "sequential" algorithms. Sequential algorithms randomize a matrix by exchanging values between existing cells ("swapping"). As you might guess, the `swap` algorithm is a sequential algorithm. One such swap is a single "iteration". If the total number of iterations, `n_iterations`, is too low, the randomized matrix won't be sufficiently randomized, and will still resemble the original matrix^[A third argument, `thin`, only applies to a small number of algorithms; for details, see `vegan::commsim()`].

If either `n_reps` or `n_iterations` are set too high, it will take overly long to finish the calculations. So our goal is to set them sufficiently high to achieve proper randomization, but not so high `cpr_rand_test()` never finishes.

### Effect of `n_iterations`

`canaper` includes a function to help determine the appropriate number of iterations, `cpr_iter_sim()`. It starts with the raw data, then shuffles it up to a maximum number of times (`n_iterations`). Each time it shuffles the matrix, it calculates the similarity between the original data and the shuffled version. We expect to see the similarity decrease with each iteration until further shuffling ceases to have an effect. You can think of this like shuffling a deck of cards; after many shuffles, further shuffling doesn't make the deck any more different from when you started.

If you have a large dataset, you may not need to inspect every iteration. The `thin` value tells the function to save only once every `thin` values (e.g., once every 100 iterations, etc.)

We will run `cpr_iter_sim()` on each dataset, starting with `biodiverse_example`.

```{r}
#| label: iter-sim-biod
biod_iter_sim_res <- cpr_iter_sim(
  comm = biod_example$comm,
  null_model = "curveball",
  n_iterations = 10000,
  thin = 10
)

biod_iter_sim_res
```

The output is simple: just two columns. But its hard to gain much insight from the raw numbers. Let's plot them.

```{r}
#| label: iter-sim-biod_plot
ggplot2::ggplot(biod_iter_sim_res, aes(x = iteration, y = similarity)) +
  geom_line() +
  labs(x = "Num. iterations", y = "% Similarity")
```

From this, we can see that the original matrix and the randomized matrix reach a maximum dissimilarity at ca. 1,000 iterations. After that, the randomized matrix doesn't become any more different with additional "mixing".

How does this compare to the *Acacia* dataset? We will need to greatly increase the number of iterations since the dataset is much larger.

```{r}
#| label: iter-sim-acacia
acacia_iter_sim_res <- cpr_iter_sim(
  comm = acacia$comm,
  null_model = "curveball",
  n_iterations = 100000,
  thin = 1000
)

ggplot2::ggplot(acacia_iter_sim_res, aes(x = iteration, y = similarity)) +
  geom_line() +
  labs(x = "Num. iterations", y = "% Similarity")
```

We notice two important differences: the number of iterations required is much higher (ca. 40,000), and the minimum % similarity is also greater (ca. 96.5%).

That is because large matrices with many zeros take more iterations, and even then still retain relatively high similarity between the original matrix and the randomized matrix. So I recommend exploring the data as above to determine the minimum number of iterations needed.

Now that we've settled on the number of iterations per random replicate, let's look into the number of replicates. 

### Effect of `n_reps`

With randomizations, there is no "right" answer, so we can't test to see that `cpr_rand_test()` produces the exact answer we're looking for. Rather, we will check that it starts to **converge on approximately the same result** once `n_reps` is high enough. 

The code below compares the percentile of observed phylogenetic diversity relative to random (`pd_obs_p_upper`, [one of the values used for calculating endemism type](https://docs.ropensci.org/canaper/articles/canape.html#classify-endemism)) between pairs of random grid-cells each generated with the same number of replicates^[Other values based on the randomizations could be checked too (e.g., values ending in `_obs_z`, `_rand_mean`, or `_rand_sd`).].

We are only running this on the `biod_example` data because the `acacia` dataset would take too long to run during a workshop.

```{r}
#| label: n-reps-biodiverse
#| warning: false
n_reps_test <- c(10, 100, 1000)

iter_sim_1 <- purrr::map(
  n_reps_test,
  ~cpr_rand_test(
    comm = biod_example$comm,
    phy = biod_example$phy,
    null_model = "curveball",
    n_iterations = 2000,
    n_reps = .x,
    tbl_out = TRUE)
  )

iter_sim_2 <- purrr::map(
  n_reps_test,
  ~cpr_rand_test(
    comm = biod_example$comm,
    phy = biod_example$phy,
    null_model = "curveball",
    n_iterations = 2000,
    n_reps = .x,
    tbl_out = TRUE)
  )

plot(iter_sim_1[[1]]$pd_obs_p_upper, iter_sim_2[[1]]$pd_obs_p_upper)
plot(iter_sim_1[[2]]$pd_obs_p_upper, iter_sim_2[[2]]$pd_obs_p_upper)
plot(iter_sim_1[[3]]$pd_obs_p_upper, iter_sim_2[[3]]$pd_obs_p_upper)
```

This gives us a visual confirmation that our results are much more consistent when the number of reps has been set to at least 1,000.

The number of reps needed is somewhat less variable than number of iterations across datasets. 1,000 is a good rule of thumb, but I still recommend testing with an analysis like this one to be sure.

## Randomization test

Now that we have decided on settings for the randomization (at least 1,000 replicates of 2,000 iterations each), we will run the randomization test.

The function to do this is canaper is `cpr_rand_test()`, which outputs a dataframe:

```{r}
#| label: rand-test
biod_example_rand_res <- cpr_rand_test(
  biod_example$comm, biod_example$phy,
  null_model = "curveball",
  n_reps = 1000, n_iterations = 2000,
  tbl_out = TRUE
)

biod_example_rand_res
```

There are a lot of columns. We can decipher what they mean though by using a few keywords: columns starting with `pd` are values of phylogenetic diversity, `rpd` is relative phylogenetic diversity, `pe` is phylogenetic endemism, etc. Next, those with `rand` refer to values from the randomization, whereas those with `obs` are the observed value.

So `pd_obs` is observed PD, `pd_rand_mean` is the mean of the random PD values, etc.

While we don't have time to go into all of these right now, but if you need to go back and look at any of the results in detail they are all there.

## Classify endemism

The next step is to classify endemism types based on the results of the randomization test. This is done with `cpr_classify_endem()`. Just give it the randomization results and it will add a column with endemism type:

```{r}
#| label: classify-endem
biod_example_canape <- cpr_classify_endem(biod_example_rand_res)

select(biod_example_canape, endem_type)
```

Let's count these to see how many times the different endemism types were observed:

```{r}
count(biod_example_canape, endem_type)
```

## Plot results

The final step is to plot the endemism categories.

When working with your own data, the geographic information may be one of various formats.

For this example dataset, information about the location of each site is embedded in the site name.

```{r}
biod_example_canape %>%
  select(site)
```

The number before the colon is position of the center of the site (its "centroid") on the x-axis and the number after is position on the y-axis. This data happens to use a projection that is designed specifically for Australia, and the numbers are in units of meters. Each site is 50 x 50 km.

So, in order to plot this, we need to split the x and y positions into separate columns:

```{r}
biod_example_canape_geo <-
  biod_example_canape %>%
  separate(site, c("long", "lat"), sep = ":") %>%
  mutate(across(c(long, lat), parse_number))

select(biod_example_canape_geo, long, lat)
```

Now we can plot the data.

```{r}
ggplot(biod_example_canape_geo) +
  geom_tile(aes(x = long, y = lat, fill = endem_type)) +
  scale_fill_manual(values = mishler_endem_cols)
```

`canaper` comes with palettes for endemism colors. Here, we have used the color scheme that appeared in the original publication of CANAPE, Mishler et al. 2014 (`mishler_endem_cols`). However, these colors may not be distinguishable to people with color vision deficiency. The `cpr_endem_cols` palette has been designed to take this into account.

```{r}
ggplot(biod_example_canape_geo) +
  geom_tile(aes(x = long, y = lat, fill = endem_type)) +
  scale_fill_manual(values = cpr_endem_cols)
```

In fact, there are several alternative color schemes available, `cpr_endem_cols_2` through `cpr_endem_cols_4`. You can try out different palettes to see which one works best for you.

Note that we used `geom_tile` because the sites are located in a regularly spaced grid. This may not be the case with your own data. You may need to use other `geom_()` functions as needed.
