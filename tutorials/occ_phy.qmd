---
title: "Obtaining occurrence and phylogeny data in R"
format: gfm
execute: 
  echo: true
fig-format: retina
output-dir: tutorials
---

```{r}
#| label: setup
#| echo: false
#| message: false
#| warning: false
library(rgbif)
library(tidyverse)
```

This tutorial demonstrates how to obtain occurrence records and phylogenies to use for spatial phylogenetics in R. As an example, we will obtain occurrence records and a phylogeny for the fern genus *Crepidomanes*.

## Obtaining occurrence data with rgbif

```{r}
#| label: current-num-records
#| echo: false
#| cache: true

gbif_n_rec <- scales::number(occ_count(), big.mark = ",")
```

[GBIF](https://www.gbif.org/) is the largest platform for accessing biodiveristy data^[As of writing, GBIF includes `r gbif_n_rec` occurrence records!], making it an excellent source of occurrence data for spatial phylogenetics.

[`rgbif`](https://docs.ropensci.org/rgbif/) is an R package that interfaces with the GBIF [API](https://en.wikipedia.org/wiki/API), allowing you to query and download GBIF data from R.

### Setup: storing login credentials

To do anything beyond simple queries with rgbif, you will need a GBIF account.

If you don't have one yet, create it now by going to <https://www.gbif.org>, clicking on "Login" in the upper-right, then clicking on "Register".

To make authentication easier, we will store login credentials in a special file called `.Renviron` outside of your project using the `usethis` package:

```{r}
#| label: setup-cred
#| eval: false
usethis::edit_r_environ("user")
```

This will open the `.Renviron` file in your editor.

Fill in your credentials like this (replacing the dummy values like 'myname' with you real data):

```
GBIF_USER=myname
GBIF_PWD=secretpassword
GBIF_EMAIL=me@myemail.com
```

Save it and restart R. Now you will be able to use `rgbif` download functionality without having to enter your login credentials every time.

### Quick searches

`occ_search()` is a quick way to get data into R (and does not require logging in), but is limited to 100,000 records per query.

```{r}
#| label: rgbif-search
#| cache: true
#| message: false
#| warning: false
library(rgbif)
library(tidyverse)

# Query for the fern genus Crepidomanes
crep_records <- occ_search(scientificName = "Crepidomanes")

# Data are stored in 'data'
crep_records$data[1:6, 1:6]

# There are a *lot* of columns: 162!
dim(crep_records$data)
```

### Downloading a complete dataset

To download a complete dataset, use `occ_download()`. This requires you provide login credentials, but since we already set them up using the `.Renviron` file, you won't have to type them in.

I assume that you are working in a project folder than includes a subdirectory called "data_raw". If not, set this up now.

```{r}
#| label: rgbif-download-show
#| eval: false

# First we need to look up the species key for Crepidomanes
crep_key <- name_backbone("Crepidomanes")$genusKey

# Send download request
gbif_download <- occ_download(
  pred("taxonKey", crep_key),
  format = "SIMPLE_CSV")

# Wait for the request to be completed (should take 2-3 minutes)
occ_download_wait(gbif_download, curlopts = list(http_version=2))

# Download the data
crep_records_path <- occ_download_get(gbif_download, "data_raw")

# Load the data into R
crep_records_raw <- occ_download_import(crep_records_path)
```

```{r}
#| label: rgbif-download-hide
#| echo: false

# First we need to look up the species key for Crepidomanes
crep_key <- name_backbone("Crepidomanes")$genusKey

# Send download request
gbif_download <- occ_download(
  pred("taxonKey", crep_key),
  format = "SIMPLE_CSV")

# Download the data
crep_records_raw <- occ_download_get(gbif_download, overwrite = TRUE) %>%
  occ_download_import()

zip_files <- list.files(here::here(), "zip")

fs::file_move(here::here(zip_files), here::here("tutorials", zip_files))
```

Note that the output of `occ_download()` includes a DOI for this dataset:

```{r}
gbif_download
```

You should visit the DOI to see what your raw dataset looks like in GBIF: <`r paste0("https://doi.org/", attr(gbif_download, "doi"))`>.

You should always **be sure to cite the DOI if you publish your study**.
If you filter the data, you may need to cite a "derived" dataset.
For more information, see [this documentation from rOpenSci](https://docs.ropensci.org/rgbif/articles/gbif_citations.html).

[GBIF has a policy](https://www.gbif.org/faq?question=for-how-long-will-does-gbif-store-downloads) of retaining datasets registered with a DOI for 6 months initially, and as long as possible once published.

### Inspect the dataset

Let's take a look at the dataset.

```{r}
#| label: inspect-gbif-data-1
#| cache: true
crep_records_raw
```

The dataset is so large it is difficult to print out to the screen. This output type (`"SIMPLE_CSV"`) includes 50 columns, which is less than when we used `occ_search()`, but it's still a lot.

The `dplyr::glimpse()` function is useful when there are lots of columns to get an idea of what each column holds.

```{r}
#| label: inspect-gbif-data-2
#| cache: true
glimpse(crep_records_raw[1, ])
```

That's better.

There are number of columns that bear mentioning^[Most of these columns are [Darwin Core terms](https://www.gbif.org/darwin-core), which is a standard for exchanging biodiversity data].

- `datasetKey` is a unique identifier for the dataset that the occurrence record belongs to. You will want to cite this for each occurrence record you use.
- `species` is the Latin binomial.
- `scientificName` is the scientific name including the taxonomic authority. This is the official name used by GBIF (the "GBIF backbone taxonomy").
- `verbatimScientificName` is the scientific name that was originally associated with this record.
- `decimalLatitude` is the latitude in decimal-degrees.
- `decimalLongitude` is the longitude in decimal-degrees.
- `basisOfRecord` describes what kind of occurrence record this is (more on that below).
- `issue` lists any possible data-quality issues associated with this record.

What is the deal with `basisOfRecord`? Let's see what values are included in this dataset:

```{r}
#| label: basis
crep_records_raw %>%
  count(basisOfRecord)
```

We see that the majority of these are `PRESERVED_SPECIMEN`; in the case of plants, those are almost certainly herbarium specimens.
You can treat such occurrence records as high-quality because they have a voucher specimen to back them up; if you really wanted to, you could theoretically track down the specimen for any of these occurrence records and inspect it.

The next most common is `HUMAN_OBSERVATION`. This means that this species was recorded as being in a particular place, but there is no voucher specimen for it. Most of these types of records come from [iNaturalist](https://www.inaturalist.org/). These should be treated as less reliable than `PRESERVED_SPECIMEN` since we can't verify them. However, only iNaturalist records that have been certified as "Research Grade"^[[According to GBIF](https://www.gbif.org/dataset/50c9509d-22c7-4a22-a47d-8c48425ef4a7), "iNaturalist observations become candidates for Research Grade when they have a photo, date, and coordinates. They become 'Research Grade' when the community agrees on an identification."] are included in GBIF, so that does offer some assurance of quality.

Another common type of `basisOfRecord` that comes up is `FOSSIL_SPECIMEN`, which are fossils as the name suggests. If you are only interested in extant organisms you probably want to exclude these. The complete list of possible values is at <https://gbif.github.io/parsers/apidocs/org/gbif/api/vocabulary/BasisOfRecord.html>.

Let's see how many species there for each type of `basisOfRecord`:

```{r}
#| label: count-species-by-obs-type
crep_records_raw %>%
  group_by(basisOfRecord) %>%
  summarize(
    n_sp = n_distinct(species)
  )
```

Most of them are preserved specimens, as expected.

How many species are only represented by `HUMAN_OBSERVATION` records? In other words, if we were to drop all `HUMAN_OBSERVATION` records, how many species would we lose?

```{r}
#| label: count-hum-obs-only
crep_records_raw %>%
  group_by(species) %>%
  add_count(basisOfRecord) %>%
  filter(basisOfRecord == "HUMAN_OBSERVATION", n == 1) %>%
  count(species)
```

For now we will leave them in, but these are the sorts of things you need to consider when working with data from GBIF.

## Cleaning spatial data with CoordinateCleaner

As mentioned in the lecture this morning, we cannot treat GBIF data at face-value; unfortunately errors may creep into the data on GBIF. These may include typos, arbitrary data points such as country centroids used when actual GPS points are lacking, etc. We don't want to include erroneous data points in our analysis, so we need to clean the data.

### Visualize the raw data

Before we clean the raw data, let's visualize it (always a good idea, regardless of what kind of data you are working with). Here we will use the `ggplot2` package included in `tidyverse`.

```{r}
# Download world map data
library(rnaturalearth)

world_map <- ne_countries(returnclass = "sf")

ggplot()+
  geom_sf(data = world_map) +
  geom_point(
    data = crep_records_raw,
    aes(x = decimalLongitude, y = decimalLatitude, color = species),
    size = 0.5) +
  theme(legend.position = "none")
```

There are too many colors to distinguish all the species so I'm not showing their names, but its still interesting to see their overall distribution patterns.

Looking at this map, are there any points that you think might be errors?

### Remove missing values

```{r}
#| echo: false
#| message: false
n_missing <- 
  crep_records_raw %>%
  transmute(long_na = is.na(decimalLongitude), lat_na = is.na(decimalLatitude)) %>%
  mutate(any_missing = long_na | lat_na) %>%
  summarize(n_missing = sum(any_missing)) %>%
  pull(n_missing)
```

Did you notice the warning message when we ran `ggplot()`? It said `r n_missing` rows containing missing values had been removed rom the plot. Clearly we can't use data that lack GPS points! The first step of cleaning the data is to remove these values.

```{r}
crep_records_no_missing <-
  crep_records_raw %>%
  # Also remove data that are missing species names
  mutate(species = na_if(species, "")) %>%
  filter(
    !is.na(species),
    !is.na(decimalLongitude),
    !is.na(decimalLatitude)
  )

crep_records_no_missing
```

That alone decreased our number of records by nearly half.

### Example CoordinateCleaner cleaning function: `cc_sea()`

So we have removed obviously unusable data points. Great! But we need to be careful that there may still be erroneous data lurking. For example, records of terrestrial plants that show up in the ocean, country centroids used instead of actual GPS points, etc. Fortunately, the `CoordinateCleaner` R package can detect these for us.

To demonstrate how this works, let's start with an example: data points in the ocean. Our study species is a terrestrial fern, so if any occurrence records show up in the middle of the ocean, they should be removed.

Here, "scale" tells `CoordinateCleaner` to use the most detailed scale possible, `10` (you could choose `50` or `110` also, but these are progressively lower resolution^[The `coordinateCleaner` help documentation mistakenly says it is the reverse order]).

```{r}
library(CoordinateCleaner)

crep_records_sea_flagged <- 
  cc_sea(
    crep_records_no_missing,
    lon = "decimalLongitude",
    lat = "decimalLatitude",
    scale = 10,
    value = "flagged"
  )
```

The output here is a logical vector of occurrence records that passed the test: `TRUE` are those that passed, and `FALSE` are those that failed.

```{r}
head(crep_records_sea_flagged)
```

However, we should not accept the results of `CoordinateCleaner` blindly. Let's dig in to these in a bit more detail.

First, filter the original records to only those that got flagged as being in the ocean:

```{r}
crep_records_sea <-
  crep_records_no_missing %>%
  filter(!crep_records_sea_flagged)
```

We'd like to plot these, but there are too many to look at them in detail on a global scale. Instead, let's see what country had the most and then zoom in there.

```{r}
count(crep_records_sea, countryCode, sort = TRUE)
```

It looks like Japan (`JP`) had by far the most. OK, let's plot those.

```{r}
# Load package needed for hi-res maps
library(rnaturalearthhires)

# Filter flagged records to just Japan
crep_records_sea_jp <- 
  crep_records_sea %>%
  filter(countryCode == "JP")

# Download map data for Japan
japan <- rnaturalearth::ne_countries(country = "japan", scale = 10, returnclass = "sf")

# Plot the flagged points
ggplot() +
  geom_sf(data = japan) +
  geom_point(
    data = crep_records_sea_jp,
    mapping = aes(x = decimalLongitude, y = decimalLatitude),
    color = "red"
  )
```

See a pattern? All (or at least the vast majority) of points that got flagged occur along coastlines or nearby small islands. So these are probably perfectly useable points, but they are close enough to the coast that they got mistakenly flagged. That is because the default `CoordinateCleaner` maps of coastlines expect very high accuracy of the GPS points. However, there is an alternate setting, a map with buffers around the coasts called "buffland". Load it with `data("buffland")`.

```{r}
data("buffland")
```

This is what it looks like when we plot it. Any points **outside** the grey area will be flagged as being in the ocean.

```{r}
# We need to load the sf package for this (more on that in a bit)
library(sf)

ggplot(st_as_sf(buffland)) +
  geom_sf(fill = "grey70")
```

As you can see, by using this map as a cutoff, we be sure that we are only excluding points in the open ocean, far from any coasts. Let's try that.

```{r}
crep_records_sea_flagged_buff <- 
  cc_sea(
    crep_records_no_missing,
    lon = "decimalLongitude",
    lat = "decimalLatitude",
    ref = buffland,
    value = "flagged"
  )
```

Many fewer records were flagged this time.

We can plot them:

```{r}
crep_records_sea_buff <-
  crep_records_no_missing %>%
     filter(crep_records_sea_flagged_buff == FALSE)

ggplot() +
  geom_sf(data = world_map) +
  geom_point(
    data = crep_records_sea_buff,
    mapping = aes(x = decimalLongitude, y = decimalLatitude),
    color = "red"
  )
```

This looks better, but you may want to zoom in to individual points, for example by querying their latitude and longitude on Google Maps.

### Full CoordinateCleaner cleaning function: `clean_coordinates()`

There are many functions starting with `cc_` that do other checks, such as `cc_dupl()` that checks for duplicates, `cc_urb()` that checks for species in urban areas, etc. You could pass your data through these individually. However, there is also a single function that can run them all at once, `clean_coordinates()`. The `tests` argument tells it which tests to conduct. Note that there is a long list of settings for the various tests. Here we are using mostly default settings, but **you should always check to make sure the settings make sense for your data**.

```{r}
crep_records_flagged <-
  clean_coordinates(
    crep_records_no_missing,
    lon = "decimalLongitude",
    lat = "decimalLatitude",
    species = "scientificName",
    tests = c(
      "capitals",     # records within 2km around country and province centroids
      "centroids",    # records within 1km of capitals centroids
      "duplicates",   # duplicated records
      "equal",        # records with equal coordinates
      "gbif",         # records within 1 degree (~111km) of GBIF headquarters
      "institutions", # records within 100m of zoo and herbaria
      "outliers",     # outliers
      "zeros",        # records with coordinates 0,0
      "urban",        # records within urban areas
      "seas"          # records in the ocean
    ),
    seas_ref = buffland,
    country_refcol = "countryCode",
    value = "spatialvalid" # result of tests are appended in separate columns
  )
```

The output is our original dataframe with one column added for each test, as well as an overall column called `.summary`.

Let's get a summary of the results.

```{r}
summary(crep_records_flagged)
```

Unfortunately the output is a bit cryptic because `CoordinateCleaner` uses very short abbreviations for column names, but we can see that duplicates (`.dpl`) account for the largest share of flagged records. This is because GBIF is an **aggregator** of databases, so it is quite possible that the same specimen shows up multiple times. Good thing we spotted these!

Let's plot the cleaned and flagged data together:

```{r}
ggplot() +
  geom_sf(data = world_map) +
  geom_point(
    data = crep_records_flagged,
    mapping = aes(x = decimalLongitude, y = decimalLatitude, color = .summary)
  )
```

Finally, let's trim the occurrence records down to just the clean data points.

```{r}
crep_records_clean <-
  crep_records_flagged %>%
  filter(.summary == TRUE) %>%
  select(-contains(".")) %>%
  as_tibble()
```

## Obtaining a phylogeny

As mentioned in the lecture, there are various sources of phylogenies, but unfortunately no "one size fits all" solution.

Since this example is for ferns, we will download a fern phylogeny that is conveniently available via the `ftolr` package.

It isn't on CRAN, so you need to use a slightly different command to install it:

```{r}
#| eval: false
install.packages(
  "ftolr",
  repos = c(
    "https://joelnitta.r-universe.dev/",
    "https://cran.rstudio.com/"
  ),
  dep = TRUE)
```

Once it's installed, we can obtain the tree with the `ft_tree()` function.

```{r}
library(ftolr)

fern_tree <- ft_tree()

fern_tree
```

## Matching names

There's not much to say about the phylogeny, since that will be highly particular to your study.

But a much more frequently encountered problem is matching names between the phylogeny and the community data. How can we do that?

First let's see how many species are in common between the two to begin with. Notice that the tree uses underscores in the species names, so we need to account for that.

```{r}
common_species <- intersect(fern_tree$tip.label, str_replace_all(crep_records_clean$species, " ", "_"))
length(common_species)
n_distinct(crep_records_clean$species)
```

Of the `r n_distinct(crep_records_clean$species)` species in the cleaned occurrence data, `r length(common_species)` are in the tree.

Are the missing species truly missing, or are they just differences in taxonomy (different synonyms)?

The names from the GBIF data are already standardized to the GBIF taxonomy.

However, the names in the tree may not conform to the GBIF taxonomy. We can query the names in the tree against the GBIF taxonomy using the `name_backbone_checklist()` function of the `rgbif` package.

The first step is to format the query. `rgbif` expects this to be in the form of a dataframe. Since we are only interested in ferns, we will set the `class` column to "Polypodiopsida". You could similarly set other taxonomic ranks like `phylum`, `order`, `family`, etc.

```{r}
# Set up query
names_query <- tibble(
  name = str_replace_all(fern_tree$tip.label, "_", " "),
  class = "Polypodiopsida"
)
```

Next, run the query using `name_backbone_checklist()`. This may take a few minutes. Note that it is actually preferrable to query full scientific names with authors, but we are skipping that because of time restrictions.

```{r}
fern_tree_names_lookup <- name_backbone_checklist(names_query)
dim(fern_tree_names_lookup)
```

As so often, the results are a large dataframe. Let's just peak at the first row to see what's going on.

```{r}
fern_tree_names_lookup %>%
  slice(1) %>%
  glimpse()
```

Important columns to note are `verbatim_name` with the original query name and `species` with the accepted binomial in GBIF.

Some other interesting columns are `status` and `matchType`. Let's see what kind of values those include:

```{r}
fern_tree_names_lookup %>% count(status)
```

`status` includes `ACCEPTED`, `DOUBTFUL`, and `SYNONYM`. `ACCEPTED` is names that are the same between the query and reference (GBIF). `SYNONYM` are names in the query that are treated as synonyms by GBIF. These are what we are trying to fix. `DOUBTFUL` are names that GBIF thinks may be invalid.

```{r}
fern_tree_names_lookup %>% count(matchType)
```

`matchType` includes `EXACT`, `FUZZY`, and `HIGHERRANK`. `EXACT` are names that could be looked up exactly as they are. `FUZZY` means that the query differed slightly from the match, but the matching algorithm considered them close enough to be recognized as the same. (We'll look into `HIGHERRANK` in a moment).

Let's see an example of a fuzzy match:

```{r}
fern_tree_names_lookup %>%
  filter(matchType == "FUZZY") %>%
  slice(1) %>%
  glimpse()
```

```{r}
#| echo: false
fuzzy_ex <-
  fern_tree_names_lookup %>%
  filter(matchType == "FUZZY") %>%
  slice(1)
```

We can see that the query name "`r fuzzy_ex$verbatim_name`" is nearly the same as, but slightly different from, the matched name "`r fuzzy_ex$species`". We can see this is likely a spelling mistake. Such slight differences are common between databases, hence the need for fuzzy matching. However, it is possible to get **false positives** with fuzzy matching too --- sometimes names that differ only by one letter are indeed different! So ideally **you should check the whole list of fuzzy matches and make sure they are correct.**

OK, what about `matchType` "HIGHERRANK"?

```{r}
fern_tree_names_lookup %>%
  filter(matchType == "HIGHERRANK") %>%
  slice(1) %>%
  glimpse()
```

```{r}
#| echo: false
hirank_ex <-
  fern_tree_names_lookup %>%
  filter(matchType == "HIGHERRANK") %>%
  slice(1)
```

In this case, the query name "`r hirank_ex$verbatim_name`" matched to a higher rank, "`r hirank_ex$genus`", but not a species. We will go ahead and exclude these because we know that all of our queries were at the species level and should have a species-level match. But ideally, **you should see if you can manually find a match for such queries**.

Now we adjust the names in the tree to match the GBIF reference taxonomy.

```{r}
fern_tree_to_gbif <-
  fern_tree_names_lookup %>%
  # Drop names that could not be match to species
  filter(!is.na(species)) %>%
  # Make a lookup table: match the original name to the gbif name
  transmute(
    original_name = str_replace_all(verbatim_name, " ", "_"),
    gbif_name = str_replace_all(species, " ", "_")
  )

# Trim the tree to only names that have matches
fern_tree_renamed <- ape::keep.tip(fern_tree, fern_tree_to_gbif$original_name)

# Rearrange the look up table to be the same order as the tips in the tree
fern_tree_to_gbif <-
  tibble(original_name = fern_tree_renamed$tip.label) %>%
  left_join(fern_tree_to_gbif, by = "original_name")

# Rename the tree tips
fern_tree_renamed$tip.label <- fern_tree_to_gbif$gbif_name
```

And finally match the tree and occurrence records by species name. Note that we are dropping any species that don't occur in both the tree and the occurrence data.

```{r}
# Find the species in common between the tree and the occurrence data
species_in_both <- intersect(
  str_replace_all(crep_records_clean$species, " ", "_"),
  fern_tree_renamed$tip.label)

# Subset tree to only species in occurrence data
crepidomanes_tree <- ape::keep.tip(fern_tree_renamed, species_in_both)

# Subset occurrence data to only species in tree
crep_records_trim_to_tree <-
  crep_records_clean %>%
  # Convert spaces in species names to underscores, to match tree
  mutate(species = str_replace_all(species, " ", "_")) %>%
  filter(species %in% crepidomanes_tree$tip.label)
```

## Aggregating points to communities

### points2comm()

The final step before conducting spatial phylogenetic analysis is to aggregate the occurrence points into grid-cells. For this, we will use the `points2comm()` function of the `phyloregion` package.

```{r}
library(phyloregion)

comm <-
  points2comm(
    dat = select(
      crep_records_trim_to_tree, species,
      decimallongitude = decimalLongitude,
      decimallatitude = decimalLatitude),
    res = 2
  )
```

The output of `points2comm` is a list including two items:

```{r}
names(comm)
```

`comm_dat` is the community matrix (here only showing the first six rows and columns):

```{r}
comm$comm_dat[1:6, 1:6]
```

`poly_shp` is the GIS layer data: the shapes of the grid-cells.

```{r}
comm$poly_shp
```

### Resolution and redundancy

Before continuing, let's take a closer look at one of the arguments used in `points2comm`, `res`. `res` is the resolution to use for the grid-cells: here, we have selected 2 degrees per side. Note that the default settings are to split up the entire earth into grid-cells, so if you set `res` to a small number (e.g., 0.01), you will end up with an extremely large (memory-intensive) data object, and R might even crash. In that case, you should use the `mask` argument to select a smaller area before generating grid-cells.

The setting for `res` also deserves careful consideration because it represents your hypothesis about what defines a community: you are saying that you consider species occurring within each grid-cell to be capable of interacting. If `res` is too large, the grid-cells may include many different habitat types that do not actually have anything to do with each other; if it is too small you may not have sufficient sampling for each grid-cell and they will mostly be empty.

One statistic we can use to help assess if our resolution is set appropriately is called "redundancy," which is one minus the ratio of number of species to the number of samples (total abundance) in each grid-cell (Garcillán, P. P., et al. 2003. J. Veg. Sci. 14: 475–486). If each species has been sampled many times, redundancy will be high, near its maximum value of 1. If each species has been sampled only a few times, redundancy will be low. If each species has only been sampled once, redundancy will be zero.

Calculating redundancy is straightforward with the output from `points2comm()`.

```{r}
redundancy_res_2 <-
  comm$poly_shp %>%
    as_tibble() %>%
    mutate(redundancy = 1 - richness/abundance)
```

We could then plot a histogram or bar chart to take a closer look:

```{r}
#| warning: false
#| message: false
ggplot(redundancy_res_2, aes(x = redundancy)) +
  geom_histogram()
```

What does this histogram tell you?

We don't have time to investigate the effects of different resolutions for this dataset, but that is something you should be sure to look into for your own analysis.

### Plotting

`phyloregion` stores the GIS data as an R object called a `SpatialPolygonsDataFrame`, or class `sp`. Recently, a newer data format called "Simple features", or class `sf`, has become popular and is gradually replacing `sp`. `sf` is the format that we have been using for plotting so far. We don't have time to go into details about these, but just know that you can convert from `sp` to `sf` with the function `st_as_sf()`. We do this below so we can plot the grid-cells on the world map. Note that we also need to deal with the Coordinate Reference System (CRS).

```{r}
# Set the CRS to match between the background map and points
poly_shp_sf <- st_as_sf(comm$poly_shp)
st_crs(poly_shp_sf) <- st_crs(world_map)

ggplot() +
  geom_sf(data = world_map) +
  geom_sf(
    data = poly_shp_sf,
    aes(fill = richness)
  ) +
  # Use a log scale because richness is highly skewed
  scale_fill_viridis_c(trans = "log10")
```

We can see that there is highest richness in SE Asia. Cool!

Since we have come this far, let's go ahead and do one more analysis that actually uses the phylogeny and community dataset together: we will calculate raw PD.

```{r}
# PD is available in the phyloregion package
pd_crep <- PD(comm$comm_dat, crepidomanes_tree)
head(pd_crep)
```

```{r}
poly_shp_sf <-
  poly_shp_sf %>%
  mutate(pd = pd_crep)

ggplot() +
  geom_sf(data = world_map) +
  geom_sf(
    data = poly_shp_sf,
    aes(fill = pd)
  ) +
  # Use a log scale because richness is highly skewed
  scale_fill_viridis_c(trans = "log10")
```

As expected, it looks very similar to species richness. We will get more into methods that account for this expected relationship tomorrow!

```{r}
ggplot(poly_shp_sf) +
  geom_point(
    aes(x = richness, y = pd)
  )
```
